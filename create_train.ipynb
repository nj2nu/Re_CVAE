{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c6ceae6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09b73c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source parameter values to use if chosen to be fixed\n",
    "fixed_vals = {'mass_1':50.0,\n",
    "        'mass_2':50.0,\n",
    "        'mc':None,\n",
    "        'geocent_time':0.0,\n",
    "        'phase':0.0,\n",
    "        'ra':1.375,\n",
    "        'dec':-1.2108,\n",
    "        'psi':0.0,\n",
    "        'theta_jn':0.0,\n",
    "        'luminosity_distance':2000.0,\n",
    "        'a_1':0.0,\n",
    "        'a_2':0.0,\n",
    "        'tilt_1':0.0,\n",
    "        'tilt_2':0.0,\n",
    "        'phi_12':0.0,\n",
    "        'phi_jl':0.0,\n",
    "        'det':['H1','L1','V1']}   \n",
    "\n",
    "bounds = {'mass_1_min':35.0, 'mass_1_max':80.0,\n",
    "        'mass_2_min':35.0, 'mass_2_max':80.0,\n",
    "        'M_min':70.0, 'M_max':160.0,\n",
    "        'geocent_time_min':0.15,'geocent_time_max':0.35,\n",
    "        'phase_min':0.0, 'phase_max':2.0*np.pi,\n",
    "        'ra_min':0.0, 'ra_max':2.0*np.pi,\n",
    "        'dec_min':-0.5*np.pi, 'dec_max':0.5*np.pi,\n",
    "        'psi_min':0.0, 'psi_max':2.0*np.pi,\n",
    "        'theta_jn_min':0.0, 'theta_jn_max':np.pi,\n",
    "        'a_1_min':0.0, 'a_1_max':0.0,\n",
    "        'a_2_min':0.0, 'a_2_max':0.0,\n",
    "        'tilt_1_min':0.0, 'tilt_1_max':0.0,\n",
    "        'tilt_2_min':0.0, 'tilt_2_max':0.0,\n",
    "        'phi_12_min':0.0, 'phi_12_max':0.0,\n",
    "        'phi_jl_min':0.0, 'phi_jl_max':0.0,\n",
    "        'luminosity_distance_min':1000.0, 'luminosity_distance_max':3000.0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f842f9dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################\n",
    "# Main tunable variables\n",
    "##########################\n",
    "\n",
    "ndata = 256                                                                    \n",
    "rand_pars = ['mass_1','mass_2','luminosity_distance']                           # parameters to randomize (those not listed here are fixed otherwise)\n",
    "inf_pars=['mass_1','mass_2']                                                   # parameters to infer\n",
    "batch_size = 64                                                                 # Number training samples shown to neural network per iteration\n",
    "weight_init = 'xavier'                                                         #[xavier,VarianceScaling,Orthogonal] # Network model weight initialization    \n",
    "n_modes=7                                                                      # number of modes in Gaussian mixture model (ideal 7, but may go higher/lower)\n",
    "initial_training_rate=1e-4                                                     # initial training rate for ADAM optimiser inference model (inverse reconstruction)\n",
    "batch_norm=True                                                                # if true, do batch normalization in all layers of neural network\n",
    "\n",
    "# pool size and pool stride should be same number in each layer\n",
    "n_filters_r1 = [33, 33]                                                        # number of convolutional filters to use in r1 network (must be divisible by 3)\n",
    "n_filters_r2 = [33, 33]                                                        # number of convolutional filters to use in r2 network (must be divisible by 3)\n",
    "n_filters_q = [33, 33]                                                         # number of convolutional filters to use in q network  (must be divisible by 3)\n",
    "filter_size_r1 = [7,7]                                                         # size of convolutional fitlers in r1 network\n",
    "filter_size_r2 = [7,7]                                                         # size of convolutional filters in r2 network\n",
    "filter_size_q = [7,7]                                                          # size of convolutional filters in q network\n",
    "drate = 0.5                                                                    # dropout rate to use in fully-connected layers\n",
    "maxpool_r1 = [1,2]                                                             # size of maxpooling to use in r1 network\n",
    "conv_strides_r1 = [1,1]                                                        # size of convolutional stride to use in r1 network\n",
    "pool_strides_r1 = [1,2]                                                        # size of max pool stride to use in r1 network\n",
    "maxpool_r2 = [1,2]                                                             # size of max pooling to use in r2 network\n",
    "conv_strides_r2 = [1,1]                                                        # size of convolutional stride in r2 network\n",
    "pool_strides_r2 = [1,2]                                                        # size of max pool stride in r2 network\n",
    "maxpool_q = [1,2]                                                              # size of max pooling to use in q network\n",
    "conv_strides_q = [1,1]                                                         # size of convolutional stride to use in q network\n",
    "pool_strides_q = [1,2]                                                         # size of max pool stride to use in q network\n",
    "n_fc = 2048                                                                      # Number of neurons in fully-connected layers\n",
    "z_dimension=100                                                                 # number of latent space dimensions of model \n",
    "n_weights_r1 = [n_fc,n_fc,n_fc]                                                     # number fully-connected layers of encoders and decoders in the r1 model (inverse reconstruction)\n",
    "n_weights_r2 = [n_fc,n_fc,n_fc]                                                     # number fully-connected layers of encoders and decoders in the r2 model (inverse reconstruction)\n",
    "n_weights_q = [n_fc,n_fc,n_fc]                                                      # number fully-connected layers of encoders and decoders q model\n",
    "##########################\n",
    "# Main tunable variables\n",
    "##########################\n",
    "\n",
    "#############################\n",
    "# optional tunable variables\n",
    "#############################\n",
    "run_label = 'ozgrav-demo_%ddet_%dpar_%dHz_run1' % (len(fixed_vals['det']),len(rand_pars),ndata) # label of run\n",
    "bilby_results_label = 'ozgrav-demo'                                             # label given to bilby results directory\n",
    "r = 2                                                                           # number (to the power of 2) of test samples to use for testing. r = 2 means you want to use 2^2 (i.e 4) test samples\n",
    "pe_test_num = 256                                                               # total number of test samples available to use in directory\n",
    "tot_dataset_size = int(1e5)                                                     # total number of training samples available to use\n",
    "tset_split = int(1e3)                                                           # number of training samples in each training data file\n",
    "save_interval = int(2e3)                                                        # number of iterations to save model and plot validation results corner plots\n",
    "num_iterations=int(5e3)+1                                                       # total number of iterations before ending training of model\n",
    "ref_geocent_time=1126259642.5                                                   # reference gps time (not advised to change this)\n",
    "load_chunk_size = 1e5                                                           # Number of training samples to load in at a time.\n",
    "samplers=['vitamin','dynesty']                                                  # Bayesian samplers to use when comparing ML results (vitamin is ML approach) dynesty,ptemcee,cpnest,emcee\n",
    "\n",
    "# Directory variables\n",
    "plot_dir=\"results/%s\" % run_label  # output directory to save results plots\n",
    "train_set_dir='training_sets_%ddet_%dpar_%dHz/tset_tot-%d_split-%d' % (len(fixed_vals['det']),len(rand_pars),ndata,tot_dataset_size,tset_split) # location of training set\n",
    "test_set_dir='test_sets/%s/four_parameter_case/test_waveforms' % bilby_results_label                                                            # location of test set directory waveforms\n",
    "pe_dir='test_sets/%s/four_parameter_case/test' % bilby_results_label                                                                            # location of test set directory Bayesian PE samples\n",
    "#############################\n",
    "# optional tunable variables\n",
    "\n",
    "# Function for getting list of parameters that need to be fed into the models\n",
    "def get_params():\n",
    "\n",
    "    # Define dictionary to store values used in rest of code \n",
    "    params = dict(\n",
    "        make_corner_plots = True,                                               # if True, make corner plots\n",
    "        make_kl_plot = True,                                                    # If True, go through kl plotting function\n",
    "        make_pp_plot = True,                                                    # If True, go through pp plotting function\n",
    "        make_loss_plot = False,                                                 # If True, generate loss plot from previous plot data\n",
    "        Make_sky_plot=False,                                                    # If True, generate sky plots on corner plots\n",
    "        hyperparam_optim = False,                                               # optimize hyperparameters for model during training using gaussian process minimization\n",
    "        resume_training=False,                                                  # if True, resume training of a model from saved checkpoint\n",
    "        load_by_chunks = True,                                                  # if True, load training samples by a predefined chunk size rather than all at once\n",
    "        ramp = True,                                                            # if true, apply linear ramp to KL loss\n",
    "        print_values=True,                                                      # optionally print loss values every report interval\n",
    "        by_channel = True,                                                      # if True, do convolutions as seperate 1-D channels, if False, stack training samples as 2-D images (n_detectors,(duration*sampling_frequency))\n",
    "        load_plot_data=False,                                                   # Plotting data which has already been generated\n",
    "        doPE = True,                                                            # if True then do bilby PE when generating new testing samples (not advised to change this)\n",
    "        gpu_num=0,                                                              # gpu number run is running on\n",
    "        ndata = ndata,                                                          \n",
    "        run_label=run_label,                                                    \n",
    "        bilby_results_label=bilby_results_label,                                \n",
    "        tot_dataset_size = tot_dataset_size,                                    \n",
    "        tset_split = tset_split,                                                \n",
    "        plot_dir=plot_dir,\n",
    "\n",
    "        # Gaussian Process automated hyperparameter tunning variables\n",
    "        hyperparam_optim_stop = int(1.5e6),                                     # stopping iteration of hyperparameter optimizer per call (ideally 1.5 million) \n",
    "        hyperparam_n_call = 30,                                                 # number of hyperparameter optimization calls (ideally 30)\n",
    "        load_chunk_size = load_chunk_size,                                      \n",
    "        load_iteration = int((load_chunk_size * 25)/batch_size),                # How often to load another chunk of training samples\n",
    "        weight_init = weight_init,                                           \n",
    "        n_samples = 1000,                                                       # number of posterior samples to save per reconstruction upon inference (default 3000) \n",
    "        num_iterations=num_iterations,                                              # total number of iterations before ending training of model\n",
    "        initial_training_rate=initial_training_rate,                         \n",
    "        batch_size=batch_size,                                                  \n",
    "        batch_norm=batch_norm,                                                  \n",
    "        report_interval=500,                                                    # interval at which to save objective function values and optionally print info during training\n",
    "        n_modes=n_modes,                                                     \n",
    "\n",
    "        # FYI, each item in lists below correspond to each layer in networks (i.e. first item first layer)\n",
    "        # pool size and pool stride should be same number in each layer\n",
    "        n_filters_r1 = n_filters_r1,                                         \n",
    "        n_filters_r2 = n_filters_r2,                                         \n",
    "        n_filters_q = n_filters_q,                                           \n",
    "        filter_size_r1 = filter_size_r1,                                     \n",
    "        filter_size_r2 = filter_size_r2,                                     \n",
    "        filter_size_q = filter_size_q,                                       \n",
    "        drate = drate,                                                       \n",
    "        maxpool_r1 = maxpool_r1,                                             \n",
    "        conv_strides_r1 = conv_strides_r1,                                   \n",
    "        pool_strides_r1 = pool_strides_r1,                                   \n",
    "        maxpool_r2 = maxpool_r2,                                             \n",
    "        conv_strides_r2 = conv_strides_r2,                                   \n",
    "        pool_strides_r2 = pool_strides_r2,                                   \n",
    "        maxpool_q = maxpool_q,                                               \n",
    "        conv_strides_q = conv_strides_q,                                     \n",
    "        pool_strides_q = pool_strides_q,                                     \n",
    "        ramp_start = 1e4,                                                       # starting iteration of KL divergence ramp (if using)\n",
    "        ramp_end = 1e5,                                                         # ending iteration of KL divergence ramp (if using)\n",
    "        save_interval=save_interval,                                            \n",
    "        plot_interval=save_interval,                                            \n",
    "        z_dimension=z_dimension,                                              \n",
    "        n_weights_r1 = n_weights_r1,                                         \n",
    "        n_weights_r2 = n_weights_r2,                                         \n",
    "        n_weights_q = n_weights_q,                                           \n",
    "        duration = 1.0,                                                         # length of training/validation/test sample time series in seconds (haven't tried using at any other value than 1s)\n",
    "        r = r,                                                                  \n",
    "        rand_pars=rand_pars,                                                    \n",
    "        corner_parnames = ['m_{1}\\,(\\mathrm{M}_{\\odot})','m_{2}\\,(\\mathrm{M}_{\\odot})','d_{\\mathrm{L}}\\,(\\mathrm{Mpc})','t_{0}\\,(\\mathrm{seconds})',r'{\\alpha}\\,(\\mathrm{rad})','{\\delta}\\,(\\mathrm{rad})'], # latex source parameter labels for plotting\n",
    "        cornercorner_parnames = ['$m_{1}\\,(\\mathrm{M}_{\\odot})$','$m_{2}\\,(\\mathrm{M}_{\\odot})$','$d_{\\mathrm{L}}\\,(\\mathrm{Mpc})$','$t_{0}\\,(\\mathrm{seconds})$',r'${\\alpha}\\,(\\mathrm{rad})$','${\\delta}\\,(\\mathrm{rad})$'], # latex source parameter labels for plotting\n",
    "        ref_geocent_time=ref_geocent_time,                                      \n",
    "        training_data_seed=43,                                                  # tensorflow training random seed number\n",
    "        testing_data_seed=44,                                                   # tensorflow testing random seed number\n",
    "        wrap_pars=[],#['phase','psi','ra'],                                         # Parameters to apply Von Mises wrapping on (not advised to change) \n",
    "        inf_pars=inf_pars,                                                      \n",
    "        train_set_dir=train_set_dir,\n",
    "        test_set_dir=test_set_dir,\n",
    "        pe_dir=pe_dir,\n",
    "        KL_cycles = 1,                                                          # number of cycles to repeat for the KL approximation\n",
    "        samplers=samplers,                                                      \n",
    "    )\n",
    "    return params\n",
    "\n",
    "\n",
    "# Save training/test parameters of run\n",
    "params=get_params()\n",
    "f = open(\"params_%s.txt\" % params['run_label'],\"w\")\n",
    "f.write( str(params) )\n",
    "f.close()\n",
    "f = open(\"params_%s_bounds.txt\" % params['run_label'],\"w\")\n",
    "f.write( str(bounds) )\n",
    "f.close()\n",
    "f = open(\"params_%s_fixed_vals.txt\" % params['run_label'],\"w\")\n",
    "f.write( str(fixed_vals) )\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa638b56",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'params' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [5]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m os\u001b[38;5;241m.\u001b[39msystem(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmkdir -p \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[43mparams\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_set_dir\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'params' is not defined"
     ]
    }
   ],
   "source": [
    "os.system('mkdir -p %s' % params['train_set_dir'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f08df25",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (igwn-py39)",
   "language": "python",
   "name": "igwn-py39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
